
# Enron Dataset Project - Udacity

Author Ramin Mohammadi

### Short Questions
__Question 1: *Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?*__


#### backgroound oon Enron Dataset:
The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.
This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.

#### Gaol of this project 
To use machine learning techniques and find out person of intrests(POI). this dataset is already labled (POI) so by trying various machine learning techniques we may be able to detect the POI.


#### data wrangling


**Length of data is: 146**

**Number of records: 5600000**

**Number of features(variables) for each user is: 21**

**Number of users which are preson oof intrest: 18 **

### ........Information about Enron:........

  COE during scandel : SKILLING JEFFREY

  Chairman : KENNETH LAY

  financial officer : ANDREW FASTOW

**considering the COE, Chairman, financial officer as POI and checking their Total payment**

      POI: SKILLING JEFFREY K has total patyment of: 8682716
      POI: LAY KENNETH L has total patyment of: 103559793
      POI: FASTOW ANDREW S has total patyment of: 2424083

We can see Lay KENNETH has the highest total amount of payment which requires to be consider for outlier detection.

                                    ****NaN/Missing values****
Looking for NaN Values in dataset, i have created a dict with name of columns and two values for each coolumn
one is total of missing values and percentage oof missing values, i am intrested to find variables with more
than 50% missing values.


| Feature | NaN per feature | NaN Percentage |
| :--- | :--: | :---:
| Loan advances | 142 | 0.97 |
| Director fees | 129 | 0.88 |
| Restricted stock deferred | 128 | 0.87 |
| Deferred income | 97 | 0.66 |
| Long term incentive | 80 | 0.54 |
| Restricted stock | 128 | 0.87|

I was able to find outliers as :

- `TOTAL`,`THE TRAVEL AGENCY IN THE PARK`,`LOCKHART EUGENE E`.

__Question 2: *What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it.*__


* __ Decision tree model results before adding new features are as: __

| Selected Features         | Score↑ |
| :----------------------   | -----: |  
| Precisions value          | 0.120  | 
| Recall value              | 0.160  |


**I have created two new features as :**

* __ 'from_poi_ratio'__
* __ 'to_poi_ratio' __


which captures the ratio based on number of times they have contacted each other, hypoothesis is we will have higher ratio between two POI. 


* __ Decisioon tree model results after adding new features are as: __

| Selected Features         | Score↑ |
| :----------------------   | -----: |  
| Precisions value          | 0.3    | 
| Recall value              | 0.2    |

Feature scalling have been done with MinMaxScaler() from sklearn, to transform features to specific range so the models will have better performance.
looking at row dataset we can see we are having variety of different ranges, so scalling is a vital job here:

| MinMaxScaller   | Precision | Recall |
| :--------- |--------|  -----: |
|    Yes   | 0.252  | 0.281 |
|    No   | 0.187  | 0.258 |

I have used 'SelectKBest' from sklearn package which will remove all expect (k) features with highest scores. i have looked through my dataset and after multiple attempt i have chose k=10, from which 2 where related too email dataset and 8 was related to finanicail dataset. following features have been used in final model

| K Value   | Precision | Recall |
| :--------- |--------|  -----: |
|    14   | 0.25  | 0.25 |
|    10   | 0.4  | 0.5 |
|    7   | 0.35  | 0.3 |
|    4   | 0.25  | 0.18 |



| Selected Features       | Score↑ |
| :---------------------- | -----: |
| exercised_stock_options | 11.422 |
| total_stock_value       | 10.763 |
| expenses                | 5.378  |
| salary                  | 10.811 |
| deferred_income         | 17.822 |
| bonus                   | 8.459  |
| loan_advances           | 8.772  |
| shared_receipt_with_poi | 6.255  |
| to_poi_ratio            | 12.939 |
| from_poi_to_this_person | 4.3912 |


loan_advances feature with score nan: 



__Question 3: *What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?*__

Following techniques have been applied to this problem and after tunning the paramaters the results are as follow: Logistic Regression, Decision Tree, Random Forest Classifier, Support Vector Machine and Gaussian Naive Bayes. After multiple attempts and tunning the parameters, following results were the best:

| Algorithm               |Precision | Recall |
| :---------------------- |--------|  -----: |
| Logistic Regression     | 0.263  | 0.312 |
| Random Forest Classifier  | 0.600  | 0.187 |
| Kmean                     | 0.142    | 0.75|
| Guassian Naive Bayes      | 0.312  | 0.312 |
| Decission Tree          | 0.444 | 0.500|

__Question 4: *What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?*__ 

Parameters tuning means to select best parameters foor the model to ooptimize its performance, not tunning the parameters can cause the model too perform less than what it can achieve (Underfittting). 

manually - by adjusting parameters manually and comparing the results of the model (percision and recall) i cam up with following parameters, however we could have used Gridsearchcv function from skleran too automate this process.

For Example: 

* __
* algorithm='auto', copy_x=True, init='k-means++', max_iter=1000, n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',    random_state=42, tol=0.1, verbose=0
* __

* bootstrap=True, class_weight=None, criterion='gini',
            max_depth=5, max_features='sqrt', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=42, verbose=0, warm_start=False 
* __

*__ with manually parameter tunning i was having results as, which after further improvment and tunning i have chose GuassianNB as final method.__

| Algorithm               | Tuned Parameter | Precision | Recall |
| :---------------------- | --------------- |-------- |  -----: |
| Logistic Regression     | tol: 0.001 | 0.29 | 0.375 |
| Logistic Regression     | tol: 0.01  | 0.35 | 0.25 |
| Logistic Regression     | tol: 0.1 | 0.32 | 0.27 |
| Random Forest Classifier  | max depth: 7 | 0.2 | 0.1 |
| Random Forest Classifier  | max depth: 9 | 0.18 | 0.25 |
| Random Forest Classifier  | max depth: 11 | 0.15 | 0.15 |

__Question 5:*What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?*__

Validation set is sample of dataset with known output which is used to check how well the model is trained, if validation set is less than test set means the model is underfit - and if the validation error is much higher than training error it can means the model is Over-fitting.
Cross-Validation is technique to evaluate the model by partitioning the original sample into training and evaluatioon set- k-fold cross validation is a common tehnique, i have used k = 7 and k = 10 and compared the results.

In final model i have K=10 which gaves me the highest Precisioon and recall values.



__Question 6:*Give at least 2 evaluation metrics, and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm's performance*__

i have used `precision` & `recall` as 2 main evaluation metrics. The best performance belongs to GaussianNB :

| Algorithm               | Precision | Recall | Accuracy |
| :---------------------- |--------|  -----: | -------- |
| Decission Tree      | 0.444  | 0.500 | 0.854 |


With a precision score of 0.33, means for each 100 positive classification(True positive + False Positive)(Person of Interest(POI)) 33 of them are truly POI and 67 of them are innocent.
Recall (sensitivity) of 1 means for each 100 positive classification(True Positive) , 100 of them were Person of Interest, means over model was able to fully detect guilty persons.



### References:
- [Introduction to Machine Learning (Udacity)](https://www.udacity.com/course/viewer#!/c-ud120-nd)
- [MITx Analytics Edge](https://www.edx.org/course/analytics-edge-mitx-15-071x-0)
- [scikit-learn Documentation](http://scikit-learn.org/stable/documentation.html)

### Files
- `data/`: dataset files and `pickle` objects
- `tools/`: helper tools and functions
- `scripts/poi_id.py`: main submission file - POI identifier
- `scripts/tester.py`: Udacity-provided file, produce test result for submission

