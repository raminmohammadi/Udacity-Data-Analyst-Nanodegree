{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict \n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import pprint\n",
    "import pymongo\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Street map for Boston\n",
    "## Reason for choosing Boston: \n",
    "### i have choosed Boston as i am living in boston and familiar with streets and locations.  it is intresting for me to see aggregated information which is harder to find by searching online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_FILE = r'D:/DATA_ANALYSIS_NANO_PLUS_DEGREE/mongoDB/Boston_Map/boston_massachusetts.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 100 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(OSM_FILE, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(OSM_FILE, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "#     output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First i parsed through boston dataset using ElementTree and count number of unique tags using count_tags function defined as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'node': 19339, 'nd': 23305, 'member': 41, 'tag': 9154, 'relation': 13, 'way': 3094, 'osm': 1})\n"
     ]
    }
   ],
   "source": [
    "def count_tags(filename):\n",
    "    counts = defaultdict(int)\n",
    "    for event , row in ET.iterparse(filename):\n",
    "        if event =='end':\n",
    "            counts[row.tag] += 1\n",
    "        row.clear()\n",
    "    return counts\n",
    "tags = count_tags(SAMPLE_FILE)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have used Key_type and Processing_file functions to look at attribute k which contains the addresses and count number of cases as following categories: 1- lower for tags which only contains lower case, 2- lower_colon for tags whihc have a semi colon between their values and 3- peoblemchars for tgas with problematic characters as defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>^;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 8043, 'lower_colon': 673, 'other': 438, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if re.search(lower , element.attrib['k']):\n",
    "            keys['lower'] += 1 \n",
    "        elif re.search(lower_colon , element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif re.search(problemchars , element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "def processing_file(filename):\n",
    "    keys = {\"lower\" : 0 , \"lower_colon\" : 0 , \"problemchars\" : 0 , \"other\" : 0}\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        keys = key_type(element , keys)\n",
    "        element.clear()\n",
    "    return keys\n",
    "\n",
    "return_keys = processing_file(SAMPLE_FILE)\n",
    "pprint.pprint(return_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining a Unique_uder function which will iterate through the dataset and find all the \"userid\" and add them to a set, so we will only keeping unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "def Unique_user(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        element.clear()\n",
    "    return users\n",
    "users = Unique_user(SAMPLE_FILE)\n",
    "print len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Street address abbreviation\n",
    "#### we have the problem which street name abbreviation is not consistence, we are using regex matching that take the  last element in the string which usually is the street types\n",
    "\n",
    "#### then using an expected and mapping to fix the street types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Avenue\", \"Boulevard\", \"Commons\", \"Court\", \"Drive\",\"Exit\", \"Lane\", \"Parkway\", \n",
    "                         \"Place\", \"Road\", \"Square\", \"Street\", \"Trail\"]\n",
    "\n",
    "mapping = {'Ave'  : 'Avenue',\n",
    "           'Ave.' : 'Avenue',\n",
    "           'Blvd' : 'Boulevard',\n",
    "           'Dr'   : 'Drive',\n",
    "           'Ext'  : 'Exit',\n",
    "           'Ln'   : 'Lane',\n",
    "           'Pkwy' : 'Parkway',\n",
    "           'rd'   :  'Road',\n",
    "           'Rd'   : 'Road',\n",
    "           'Rd.'  : 'Road',\n",
    "           'St,'  : 'Street',\n",
    "           'st'   : 'Street',\n",
    "           'st,'  : 'Road',\n",
    "           'St.'  : 'Street',\n",
    "           'St'   : 'Street',\n",
    "           'ST'   : 'Street',\n",
    "           'street' :\"Street\",\n",
    "           'Ct'   : \"Court\",\n",
    "           'Cir'  : \"Circle\",\n",
    "           'Cr'   : \"Court\",\n",
    "           'ave'  : 'Avenue',\n",
    "           'Hwg'  : 'Highway',\n",
    "           'Hwy'  : 'Highway',\n",
    "           'pl'   : 'Place',\n",
    "           'Sq.'  : 'Square',\n",
    "           'Sq,'  : 'Square',\n",
    "           'Sq'   : \"Square\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_street_names function search the input string for the regex. If there is a match and it is not within the \"expected\" list, add the match as a key and add the string to the set.\n",
    "is_address function looks at the attribute k if k=\"addre:street\" audit functio will return the list that match previous two functions. \n",
    "\n",
    "We make function check_zipcode to find a list of zip code with incorrect 5 digit formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_street_names(street_types, street_name):\n",
    "    temp = street_type_re.search(street_name)\n",
    "    if temp:\n",
    "        street_type = temp.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "def is_address(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def checK_zipcode(wrong_zipcodes, zipcode_input):\n",
    "    firstDigits = zipcode_input[0:2]\n",
    "    \n",
    "    if firstDigits != 02:\n",
    "        wrong_zipcodes[firstDigits].add(zipcode_input)\n",
    "    elif not firstDigits.isdigit():\n",
    "        wrong_zipcodes[firstDigits].add(zipcode_input)\n",
    "        \n",
    "def is_zipcode(element):\n",
    "    return(element.attrib['k'] == \"addr:postcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape_element function will return the list that match previous two functions, and also for Proccess_zipcode we create a dict which holds zipcodes with wrong value based on check_zip_code function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(osm_filename):\n",
    "    osm_file = open(osm_filename , \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_address(tag):\n",
    "                    check_street_names(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\n",
    "def proccess_zipcode(osmFile):\n",
    "    osmfile = open(osmFile, \"r\")\n",
    "    wrong_zipcode = defaultdict(set)\n",
    "    for event, element in ET.iterparse(osmFile, events = (\"start\" , )):\n",
    "        if element.tag == \"node\" or element.tag == \"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    checK_zipcode(wrong_zipcode,tag.attrib['v'])\n",
    "    return wrong_zipcode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streets = shape_element(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Zipcodes = proccess_zipcode(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using pretty print the output of the shape_element and process_zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Park': set(['Austin Park', 'Greenough Park']),\n",
      " 'St': set(['Cambridge St', 'Everett St', 'Stewart St', 'Waverly St']),\n",
      " 'St.': set(['Albion St.']),\n",
      " 'Way': set(['Artisan Way'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(streets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': set(['01125', '01238', '01240', '01250', '01821', '01854', '01944']),\n",
      " '02': set(['02026',\n",
      "            '02026-5036',\n",
      "            '02043',\n",
      "            '02108',\n",
      "            '02109',\n",
      "            '02110',\n",
      "            '02110-1301',\n",
      "            '02111',\n",
      "            '02113',\n",
      "            '02114',\n",
      "            '02114-3203',\n",
      "            '02115',\n",
      "            '02116',\n",
      "            '02118',\n",
      "            '02119',\n",
      "            '02120',\n",
      "            '02121',\n",
      "            '02122',\n",
      "            '02124',\n",
      "            '02125',\n",
      "            '02126',\n",
      "            '02127',\n",
      "            '02128',\n",
      "            '02129',\n",
      "            '02130',\n",
      "            '02130-4803',\n",
      "            '02131',\n",
      "            '02131-3025',\n",
      "            '02131-4931',\n",
      "            '02132',\n",
      "            '02132-1239',\n",
      "            '02132-3226',\n",
      "            '02134',\n",
      "            '02134-1305',\n",
      "            '02134-1306',\n",
      "            '02134-1307',\n",
      "            '02134-1311',\n",
      "            '02134-1312',\n",
      "            '02134-1313',\n",
      "            '02134-1316',\n",
      "            '02134-1317',\n",
      "            '02134-1318',\n",
      "            '02134-1319',\n",
      "            '02134-1321',\n",
      "            '02134-1322',\n",
      "            '02134-1327',\n",
      "            '02134-1409',\n",
      "            '02134-1420',\n",
      "            '02134-1433',\n",
      "            '02134-1442',\n",
      "            '02135',\n",
      "            '02136',\n",
      "            '02136-2460',\n",
      "            '02138',\n",
      "            '02138-1901',\n",
      "            '02138-2701',\n",
      "            '02138-2706',\n",
      "            '02138-2724',\n",
      "            '02138-2735',\n",
      "            '02138-2736',\n",
      "            '02138-2742',\n",
      "            '02138-2762',\n",
      "            '02138-2763',\n",
      "            '02138-2801',\n",
      "            '02138-2901',\n",
      "            '02138-2903',\n",
      "            '02138-2933',\n",
      "            '02138-3003',\n",
      "            '02138-3824',\n",
      "            '02139',\n",
      "            '02140',\n",
      "            '02140-1340',\n",
      "            '02140-2215',\n",
      "            '02141',\n",
      "            '02142',\n",
      "            '02143',\n",
      "            '02144',\n",
      "            '02145',\n",
      "            '02148',\n",
      "            '02149',\n",
      "            '02150',\n",
      "            '02151',\n",
      "            '02152',\n",
      "            '02155',\n",
      "            '02159',\n",
      "            '02169',\n",
      "            '02170',\n",
      "            '02171',\n",
      "            '02174',\n",
      "            '02184',\n",
      "            '02186',\n",
      "            '02189',\n",
      "            '02190',\n",
      "            '02191',\n",
      "            '02199',\n",
      "            '02201',\n",
      "            '02205',\n",
      "            '02210',\n",
      "            '02215',\n",
      "            '02284-6028',\n",
      "            '0239',\n",
      "            '02445',\n",
      "            '02445-5841',\n",
      "            '02445-7638',\n",
      "            '02446',\n",
      "            '02458',\n",
      "            '02459',\n",
      "            '02467',\n",
      "            '02472',\n",
      "            '02474',\n",
      "            '02474-8735',\n",
      "            '02476',\n",
      "            '02478']),\n",
      " 'MA': set(['MA', 'MA 02116', 'MA 02118', 'MA 02186'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(Zipcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing the street abbreviation and zipcodes based on the condition which we have defined\n",
    "### and using -----Updated To----> to show what we have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin Park --------- Updated To ---------> Austin Park\n",
      "Cambridge St --------- Updated To ---------> Cambridge Street\n",
      "Everett St --------- Updated To ---------> Everett Street\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def fix_street_name(name, mapping, regex):\n",
    "    temp = regex.search(name)\n",
    "    if temp:\n",
    "        street_type = temp.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(regex , mapping[street_type] , name)\n",
    "    return name\n",
    "for street_type, roads in streets.iteritems():\n",
    "    for road in list(roads)[1:3]:\n",
    "        update_name = fix_street_name(road, mapping, street_type_re)\n",
    "        print road + \" --------- Updated To ---------> \" + update_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note That we did not have any Zipcode in Sample file which needed to be change so i have used original data for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA 02116 -----Chnaged To-----> 02116\n",
      "MA 02118 -----Chnaged To-----> 02118\n",
      "MA 02186 -----Chnaged To-----> 02186\n"
     ]
    }
   ],
   "source": [
    "def Fix_zipcodes(zipcode):\n",
    "    testNum = re.findall('[a-zA-Z]*' , zipcode)\n",
    "    if testNum:\n",
    "        testNum = testNum[0]\n",
    "    testNum.strip()\n",
    "    if testNum == \"MA\":\n",
    "        changedZip = (re.findall(r'\\d+', zipcode))\n",
    "        if changedZip:\n",
    "            if changedZip.__len__() == 2:\n",
    "                return (re.findall(r'\\d+', zipcode))[0] + \"-\" +(re.findall(r'\\d+', zipcode))[1]\n",
    "            else:\n",
    "                return (re.findall(r'\\d+', zipcode))[0]\n",
    "            \n",
    "for zipcodes, styles in Zipcodes.iteritems():\n",
    "    for style in styles:\n",
    "        better_style = Fix_zipcodes(style)\n",
    "        if better_style != None:\n",
    "            print style, \"-----Chnaged To----->\", better_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following condition has been considred to transform data from XML to JSON\n",
    "- you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings. \n",
    "- if second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it\n",
    "  same as any other tag.\n",
    "  \n",
    "  \n",
    "### and i only print out the records which have address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "address_regex = re.compile(r'^addr\\:')\n",
    "street_regex = re.compile(r'^street')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def is_address(elem):\n",
    "    if elem.attrib['k'][:5] == \"addr:\":\n",
    "        return True\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        address_info = {}\n",
    "        nd_info = []\n",
    "        #pprint.pprint(element.attrib)\n",
    "        node[\"type\"] = element.tag\n",
    "        node[\"id\"] = element.attrib[\"id\"]\n",
    "        if \"visible\" in element.attrib.keys():\n",
    "            node[\"visible\"] = element.attrib[\"visible\"]\n",
    "        if \"lat\" in element.attrib.keys():\n",
    "            node[\"pos\"] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "        node[\"created\"] = {\"version\": element.attrib['version'],\n",
    "                            \"changeset\": element.attrib['changeset'],\n",
    "                            \"timestamp\": element.attrib['timestamp'],\n",
    "                            \"uid\": element.attrib['uid'],\n",
    "                            \"user\": element.attrib['user']}\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            #print tag.attrib\n",
    "            p = problemchars.search(tag.attrib['k'])\n",
    "            if p:\n",
    "                #print \"PROBLEM:\", p.group()\n",
    "                continue\n",
    "            elif is_address(tag):\n",
    "                if \":\" in tag.attrib['k'][5:]:\n",
    "                    #print \"Bad Address:\", tag.attrib['k'], \"--\", tag.attrib['v']\n",
    "                    continue\n",
    "                else:\n",
    "                    address_info[tag.attrib['k'][5:]] = tag.attrib['v']\n",
    "                    #print \"Good Address:\", tag.attrib['k'], \"--\", tag.attrib['v']\n",
    "            else:\n",
    "                node[tag.attrib['k']] = tag.attrib['v']\n",
    "                #print \"Outside:\", tag.attrib['k'], \"--\", tag.attrib['v']\n",
    "        if address_info != {}:\n",
    "            node['address'] = address_info\n",
    "        for tag2 in element.iter(\"nd\"):\n",
    "            nd_info.append(tag2.attrib['ref'])\n",
    "            #print tag2.attrib['ref']\n",
    "        if nd_info != []:\n",
    "            node['node_refs'] = nd_info\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el and 'address' in el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original OSM file is 434.862953 MB\n",
      "The JSON file is 3.847137 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print 'The original OSM file is {} MB'.format(os.path.getsize(OSM_FILE)/1.0e6) # convert from bytes to megabytes\n",
    "print 'The JSON file is {} MB'.format(os.path.getsize(OSM_FILE + \".json\")/1.0e6) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using MongoClient and inserting the data on local host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = 'openstreetmap'\n",
    "\n",
    "def insert_map(infile , db):\n",
    "    map = process_map(infile)\n",
    "    \n",
    "    for row in map :\n",
    "        db.map.insert_one(row)\n",
    "    \n",
    "    \n",
    "# Connect to Mongo DB\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client[db_name]\n",
    "insert_map(OSM_FILE, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('58e3b20aa193831420864e15'),\n",
      " u'address': {u'city': u'Boston',\n",
      "              u'housenumber': u'599',\n",
      "              u'state': u'MA',\n",
      "              u'street': u'Old Colony Avenue'},\n",
      " u'attribution': u'Office of Geographic and Environmental Information (MassGIS)',\n",
      " u'created': {u'changeset': u'45689302',\n",
      "              u'timestamp': u'2017-01-31T16:41:55Z',\n",
      "              u'uid': u'290680',\n",
      "              u'user': u'wheelmap_visitor',\n",
      "              u'version': u'6'},\n",
      " u'id': u'69482495',\n",
      " u'line': u'Red',\n",
      " u'massgis:geom_id': u'26',\n",
      " u'name': u'JFK/UMass',\n",
      " u'operator': u'MBTA',\n",
      " u'pos': [42.32074, -71.052486],\n",
      " u'railway': u'station',\n",
      " u'source': u'massgis_import_v0.1_20071013192438;massgis_import_v0.1_20071013193425',\n",
      " u'type': u'node',\n",
      " u'website': u'http://www.mbta.com/',\n",
      " u'wheelchair': u'yes',\n",
      " u'wikipedia': u'en:JFK/UMass (MBTA station)'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(db.map.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing number of Nodes and Ways available in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes:  17486\n",
      "Numbre of Ways:  41356\n"
     ]
    }
   ],
   "source": [
    "print \"Number of Nodes: \" , db.map.find({'type' : 'node'}).count()\n",
    "print \"Numbre of Ways: \" , db.map.find({'type' : 'way'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of unique user exist in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.map.distinct('created.user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 user based on number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'COUNT': 16751, u'_id': u'jremillard-massgis'},\n",
      " {u'COUNT': 11697, u'_id': u'ryebread'},\n",
      " {u'COUNT': 1974, u'_id': u'amillar'},\n",
      " {u'COUNT': 1876, u'_id': u'srevilak'},\n",
      " {u'COUNT': 1701, u'_id': u'mterry'},\n",
      " {u'COUNT': 1512, u'_id': u'Shannon Kelly'},\n",
      " {u'COUNT': 1414, u'_id': u'woodpeck_repair'},\n",
      " {u'COUNT': 1176, u'_id': u'crschmidt'},\n",
      " {u'COUNT': 1064, u'_id': u'Peter Dobratz'},\n",
      " {u'COUNT': 1043, u'_id': u'morganwahl'}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(db.map.aggregate([{'$group' : {'_id' : '$created.user',\n",
    "                                  'COUNT' :{\"$sum\" : 1}}},\n",
    "                                    {\"$sort\" : {\"COUNT\" : -1}},\n",
    "                                    {\"$limit\" : 10}])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top 10 amenities in Boston\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'school', u'count': 2324},\n",
      " {u'_id': u'restaurant', u'count': 2121},\n",
      " {u'_id': u'library', u'count': 1960},\n",
      " {u'_id': u'fire_station', u'count': 721},\n",
      " {u'_id': u'cafe', u'count': 714},\n",
      " {u'_id': u'place_of_worship', u'count': 539},\n",
      " {u'_id': u'fast_food', u'count': 469},\n",
      " {u'_id': u'police', u'count': 336},\n",
      " {u'_id': u'university', u'count': 336},\n",
      " {u'_id': u'hospital', u'count': 315}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(db.map.aggregate([{'$match': {'amenity': {'$exists': 1}}}, \\\n",
    "                                {'$group': {'_id': '$amenity', \\\n",
    "                                            'count': {'$sum': 1}}}, \\\n",
    "                                {'$sort': {'count': -1}}, \\\n",
    "                                {'$limit': 10}])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 6 restaurant type in Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Count': 476, u'Food': None},\n",
      " {u'Count': 168, u'Food': u'american'},\n",
      " {u'Count': 147, u'Food': u'pizza'},\n",
      " {u'Count': 119, u'Food': u'chinese'},\n",
      " {u'Count': 112, u'Food': u'italian'},\n",
      " {u'Count': 112, u'Food': u'mexican'}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(db.map.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1},\n",
    "                                 \"amenity\":\"restaurant\",}},      \n",
    "                      {\"$group\":{\"_id\":{\"Food\":\"$cuisine\"},\n",
    "                                 \"count\":{\"$sum\":1}}},\n",
    "                      {\"$project\":{\"_id\":0,\n",
    "                                  \"Food\":\"$_id.Food\",\n",
    "                                  \"Count\":\"$count\"}},\n",
    "                      {\"$sort\":{\"Count\":-1}}, \n",
    "                      {\"$limit\":6}])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 building types in Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'yes', u'count': 32186},\n",
      " {u'_id': u'apartments', u'count': 3115},\n",
      " {u'_id': u'house', u'count': 2338},\n",
      " {u'_id': u'commercial', u'count': 1505},\n",
      " {u'_id': u'university', u'count': 980}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(db.map.aggregate([\n",
    "       {'$match': {'building': { '$exists': 1}}}, \n",
    "        {'$group': {'_id': '$building',\n",
    "                    'count': {'$sum': 1}}}, \n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 5}])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list of Convinence Stores sort by number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Count': 28, u'Name': u'Tedeschi Food Shops'},\n",
      " {u'Count': 21},\n",
      " {u'Count': 14, u'Name': u'7-Eleven'},\n",
      " {u'Count': 14, u'Name': u'Neighborhood Market'},\n",
      " {u'Count': 7, u'Name': u'Shop and Go American and Spanish Groceries'}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(db.map.aggregate([{\"$match\":{\"shop\":{\"$exists\":1},\n",
    "                        \"shop\":\"convenience\"}},\n",
    "            {\"$group\":{\"_id\":{\"City\":\"$city\", \"Name\":\"$name\"},\n",
    "                       \"count\":{\"$sum\":1}}},\n",
    "            {\"$project\": {'_id':0, \"City\":\"$_id.City\",\n",
    "                          \"Name\":\"$_id.Name\", \"Count\":\"$count\"}},\n",
    "            {\"$sort\":{\"Count\":-1}},\n",
    "                                     {\"$limit\" : 5}\n",
    "            ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Ideas to improve data quality of Boston OSM:\n",
    "### while auditing the dataset i have noticed there are minor error caused by human input, the dataset is fairly well-cleaned. by using a logical approach we can have a clean data set which can be used for further analysis specially for recomendation engine system which based on the number of posts for the given amenity and existing information can recomend to a user which amnity per location is better and more accesible. \n",
    "\n",
    "### Ideas to improve the quality of the data: \n",
    "#### - user enters this info through web and need to enter information manually, if user could use cellphone and location automatically entered woud improve the quality of the data.\n",
    "#### - in case of using web page to enter the information manually we could have asked for user permission to track the location through chroom and update info automatically.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
